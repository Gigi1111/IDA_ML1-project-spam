{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IDA_ML_ss22 Semester Project: Project 5 -- Spam\n\nAuthor: 815202 Chung-Fan Tsai\n\nProblem: \nYou have been hired by the IT department of a medium-sized company to train an email\nspam filter which should mark the incoming emails of all employees as spam or non-spam.","metadata":{}},{"cell_type":"markdown","source":"# Problem Formalisation\n\nTask (T): Classify an email into spam or not spam\n\nExperience (E): A corpus of extracted employee emails (size=10,000) labeled spam or not spam\n\nPerformance (P): Maximizing True Positive (\"identify a maximum number of spam emails\"), with False Positive Rate less than 0.2% (\"a maximum of 0.2% of all legitimate\nemails being classified incorrectly\")\n\n## Problem Analysis\n\nAs our data is fully labeled in discrete values (+1, -1), it is better suited for supervised learning and training a classificatier.\n\n## Target Variable (Label)\n2 classes, +1 stands for spam, −1 non-spam\n\n## Input Attributes\nEmails in the data have been converted to bag-of-words representation, with a total of 57,173 different unique words (or features). However, we do not have the lookup table of what word each index indicates, we also have no knowledge of how data preprocessing was conducted (undercasing? stopword removing?).\n","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration and Visualization\n\nmissing values (decide how to deal with them and provide a rationale for your decision) or the quantity of data for each label.\n\nWe first want to import python packages we need for this project, and read into data from the emails.mat file for exploration.","metadata":{}},{"cell_type":"code","source":"# general pacakges\nimport scipy.io \nimport numpy as np \nimport pandas as pd\nimport scipy.sparse\nimport os\nimport json\nimport random\nimport time\nimport yaml\nimport math\n\n# packages to save RAM when running module on Jupyter\nimport gc \n\n# plotting packages\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import rcParams\nrcParams['figure.figsize'] = 14, 8\n\n# normalization\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import normalize\n\n# model related packages\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import PrecisionRecallDisplay\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.utils import class_weight\nfrom sklearn.linear_model import Perceptron\n\n\n# normalization\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import normalize\n\n# Precision/recall curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\n\n# torch packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\n\nSEED = 8 # after some testing this one seems to work very well\nrandom.seed(SEED)\n\ndata = scipy.io.loadmat('../input/emails/emails.mat') \nprint(data)\n","metadata":{"id":"ewLEskhtUBtO","execution":{"iopub.status.busy":"2022-09-06T17:10:42.323686Z","iopub.execute_input":"2022-09-06T17:10:42.324881Z","iopub.status.idle":"2022-09-06T17:10:42.463317Z","shell.execute_reply.started":"2022-09-06T17:10:42.324832Z","shell.execute_reply":"2022-09-06T17:10:42.462140Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# we first look at how X and Y are stored (data type) and shaped\nX_raw = data['X']\nY_raw = data['Y']\nprint(X_raw, type(X_raw), X_raw.shape)\nprint(Y_raw, type(Y_raw), Y_raw.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:10:42.464592Z","iopub.execute_input":"2022-09-06T17:10:42.465345Z","iopub.status.idle":"2022-09-06T17:10:42.508563Z","shell.execute_reply.started":"2022-09-06T17:10:42.465304Z","shell.execute_reply":"2022-09-06T17:10:42.507178Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"So we know X is a sparse matrix of 57173 x 10000, we can use todense to decompress it to turn it into a numpy array. As we want rows to represent email entry and column for features, we'll transpose it.\nFor Y, we want to have rows of individual label to match the email etnries in X, so we'll reshape it:","metadata":{}},{"cell_type":"code","source":"X, Y = np.asarray(data['X'].todense().transpose()), data['Y'].reshape(-1)\nprint(type(X), X.shape, type(Y), Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:10:42.510282Z","iopub.execute_input":"2022-09-06T17:10:42.510773Z","iopub.status.idle":"2022-09-06T17:10:47.847917Z","shell.execute_reply.started":"2022-09-06T17:10:42.510725Z","shell.execute_reply":"2022-09-06T17:10:47.846637Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n\n# # split the data\n# # X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size= 0.9, random_state = SEED)\n\n# # train, test, validation set split we choose to be 70% 15% 15%\n# # first split the data in training and remaining dataset\n# # X_train, X_rest, y_train, y_rest = train_test_split(X,Y, train_size=0.7, random_state = SEED)\n# # # then define valid_size and test set=0.5 \n# # X_valid, X_test, y_valid, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state = SEED)\n\n\n# # stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter \n# X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.7 , stratify=Y,random_state=SEED)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:47:37.126397Z","iopub.execute_input":"2022-09-06T16:47:37.126776Z","iopub.status.idle":"2022-09-06T16:47:37.133051Z","shell.execute_reply.started":"2022-09-06T16:47:37.126743Z","shell.execute_reply":"2022-09-06T16:47:37.131037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Data analysis\n* check missing or erroneous data\n* understand units/presentation\n* check if feature selection is necessary","metadata":{"id":"PWGCUPOYb43F"}},{"cell_type":"markdown","source":"We know that X is in BOW presentation, but just in case, we should check if there are none +1/-1 values and or if there's nan entries, same goes for Y:","metadata":{}},{"cell_type":"code","source":"print(\"--------- X ---------\")\nprint(\"How many values/count are < 0, meaning erroreous count?\",X[X < 0].size)\nprint(\"Is there any entry that is not a numpy array or is empty, meaning missing entry?\",len([1 for x in X if not isinstance(X, np.ndarray)])>0 or len([1 for x in X if len(X)<1])>0)\n\nprint(\"--------- Y ---------\")\nprint(\"How many values/count are < 0, meaning erroreous count?\",Y[abs(Y)!= 1].size)\nprint(\"Is there any entry that is nan, meaning missing entry?\",any(np.isnan(y) for y in Y.flatten()))\n","metadata":{"id":"dJatmjh_aeiU","outputId":"d53abb16-c36b-4439-c7bb-8bbe66cb583d","execution":{"iopub.status.busy":"2022-09-06T16:54:52.039297Z","iopub.execute_input":"2022-09-06T16:54:52.039744Z","iopub.status.idle":"2022-09-06T16:54:52.765018Z","shell.execute_reply.started":"2022-09-06T16:54:52.039708Z","shell.execute_reply":"2022-09-06T16:54:52.763658Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Even though we don't know what the attributes are as in words, we still want to get a general idea of the size of emails, how frequent can a word appear in one email...etc:","metadata":{}},{"cell_type":"code","source":"print(\"--------- X ---------\")\nprint(\"How many labels are == 0, meaning how sparse?\",X[X == 0].size)\nprint(\"How many labels are == 1, meaning a word appears only once?\",X[X == 1].size)\nprint(\"How many labels are > 1, meaning how many times a word appears more than once?\",X[X > 1].size)\n\nX_dataframe = pd.DataFrame(X)\n\nprint(\"Most frequent word appeared how many times?\",X_dataframe.max(numeric_only=True).max())\nprint(\"Minimum word count should be?\", X_dataframe.min(numeric_only=True).min())\n\nmax_idx = X_dataframe.max(numeric_only=True).idxmax()\nprint(\"Which word/column has the max word count?\", max_idx)\nprint(\"Is it true that that's the max value?\", X_dataframe[max_idx].max() == X_dataframe.max(numeric_only=True).max())\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:56:31.645527Z","iopub.execute_input":"2022-09-06T16:56:31.645883Z","iopub.status.idle":"2022-09-06T16:56:45.154441Z","shell.execute_reply.started":"2022-09-06T16:56:31.645852Z","shell.execute_reply":"2022-09-06T16:56:45.153071Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"So we learn that in our BoW presentation, word count range is [0, 1343], with the 23712th word appears the most in one email","metadata":{}},{"cell_type":"code","source":"word_per_email = np.sum(X, axis=1)\nword_per_email_dataframe = pd.DataFrame(word_per_email)\nprint(word_per_email_dataframe.describe())\n","metadata":{"id":"h-114T2ihuVW","outputId":"dc5d4a97-7c73-4e83-c518-85fedb33471a","execution":{"iopub.status.busy":"2022-09-06T16:58:13.657228Z","iopub.execute_input":"2022-09-06T16:58:13.657764Z","iopub.status.idle":"2022-09-06T16:58:14.279075Z","shell.execute_reply.started":"2022-09-06T16:58:13.657721Z","shell.execute_reply":"2022-09-06T16:58:14.276952Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Turning word per email count into a dataframe and get a summary of it, we know that the length of each email is in the range of [4,13933], and the average amount of words per email is 562.\n\nTo see whether there are a lot of emails with few words or long essays, we want to see the distribution of it in a different way:","metadata":{}},{"cell_type":"code","source":"num_of_words = [5,10,30,100,500,1000,5000,10000]\nfor num in num_of_words:\n    if num < 1001:\n        print(\"How many emails have less than \",num,\" words?\", word_per_email[word_per_email < num].size)\n    else:\n        print(\"How many emails have more than \",num,\" words?\", word_per_email[word_per_email > num].size)\n","metadata":{"id":"cUBfq5mziOcK","outputId":"afd0464f-cd25-454f-df62-904d8cbf1905","execution":{"iopub.status.busy":"2022-09-06T16:59:45.635406Z","iopub.execute_input":"2022-09-06T16:59:45.637089Z","iopub.status.idle":"2022-09-06T16:59:45.647816Z","shell.execute_reply.started":"2022-09-06T16:59:45.637024Z","shell.execute_reply":"2022-09-06T16:59:45.646489Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"So we know most emails have the expected amount of words (not too few, not too many).\n\nThen we also want to know how many email extracted/labeled is spam:","metadata":{}},{"cell_type":"code","source":"print(\"There are \",Y[Y==-1].size, \" non-spam emails, meaning \", Y[Y==-1].size/Y.size*100,\"% of the emails in data are non-spam.\")\nprint(\"There are \",Y[Y==1].size, \" spam emails, meaning \", Y[Y==1].size/Y.size*100,\"% of the emails in data are spam.\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:02:18.229309Z","iopub.execute_input":"2022-09-06T17:02:18.229788Z","iopub.status.idle":"2022-09-06T17:02:18.238719Z","shell.execute_reply.started":"2022-09-06T17:02:18.229748Z","shell.execute_reply":"2022-09-06T17:02:18.237502Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection \nWhile we have a lot of features (or unique word counts), we cannot really perform feature selection by removing or combing some attributes (like selecting spammy word combinations) as we do not has access to the original words. That being said, we still want to check if there are very common words that appear in many emails:","metadata":{"id":"lefUf0cslro-"}},{"cell_type":"code","source":"# check if there is one word feature that appears in all emails\ncount = 0\nfor col in X_dataframe.columns:\n    if (X_dataframe[col] != 0).all():\n        count += 1\nprint(\"Number of words that appear in every email: \", count)\n\n# check if there are words/features that appear in A LOT of the emails\ncount_list = []\nfor col in X_dataframe.columns:\n    count_list.append(X_dataframe[col][X_dataframe[col] > 0].size)\n\nprint(\"Most document frequent word appears in how many emails? \", max(count_list))\n\nword_doc_freq = [5000,7500,9000, 9500]\nfor freq in word_doc_freq:\n    print(\"How many words appear in more than \",freq, \" number of emails? \", len([i for i in count_list if i > freq]))","metadata":{"id":"6h4COdIYlE-P","outputId":"dc71cb6f-5417-43de-e430-867cca843620","execution":{"iopub.status.busy":"2022-09-06T17:04:00.862544Z","iopub.execute_input":"2022-09-06T17:04:00.863050Z","iopub.status.idle":"2022-09-06T17:05:05.537773Z","shell.execute_reply.started":"2022-09-06T17:04:00.863012Z","shell.execute_reply":"2022-09-06T17:05:05.535690Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"From our word document frequency study, we learn that some 25 words do appear in more than half of the emails, but very few of them appear in more than 3/4 of the emails. We assume the ones that do (only 4 of them) are really common words (even though we aren't sure whether stop words were filtered out before being turned into BoW presentation. We assume that as we have around 80% spam emails, those common words could also be words frequently used in spam mails.\n\nAs we do not have the original emails to confirm this, we will not exclude any word/feature (regardless of its frequency). \n\nAnd as we got features in the form of BoW, we do not know the order of them, so there's no point of doing n-gram or create new composit feature. ","metadata":{"id":"H8I_F68tnr4m"}},{"cell_type":"markdown","source":"## Model Selection\n\n\nNow that we've checked there's no missing value and probably no errorneous values.\nAnd we are ok with the units and presentation of the data\nas the words are already inputted as BoW presentation,\nwe do not have to preprocess them. It's time to decide a model\nbecause they are all labeled, and target value is binary (discrete) and not ranked/ordered nor structured, we are going to perform supervised classification.\n\nTo utilize some models we've learned in the course, we'll apply the following models:\n* Decision Tree\n* Random Forest\n* Linear Regression\n* MLP","metadata":{"id":"itRj1YGGku_O"}},{"cell_type":"markdown","source":"## Data split\n\nWe'll be training with 70% of the data, use validation set of 15% to tune, and 15% of test to evaluate. The reason for this split is we want as many data to train but also enough to tune and test.","metadata":{}},{"cell_type":"code","source":"# split the data, as we have very imbalanced data, we use stratify to make sure the split includes same proportion of both classes\nX_train, X_rest, y_train, y_rest = train_test_split(X, Y, train_size= 0.7, stratify=Y, random_state = SEED)\nX_test, X_validation, y_test, y_validation = train_test_split(X_rest, y_rest, train_size= 0.5, stratify=y_rest, random_state = SEED)\n\nprint(X_train.shape, X_validation.shape, X_test.shape)\n\n\n# for spam detection, it's worse to identify legitimate mail as spam (false positive)\n# out of all non-spam emails, less than .2% of them can be mislabeled\ndef countFalsePositiveRate(pred,label, non_spam = -1):\n    return (((label == non_spam) & (pred == 1)).sum()) / len(label==non_spam) * 100\n\n\nfprs, scores = [], []","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:28:33.083203Z","iopub.execute_input":"2022-09-06T17:28:33.084718Z","iopub.status.idle":"2022-09-06T17:28:37.074989Z","shell.execute_reply.started":"2022-09-06T17:28:33.084656Z","shell.execute_reply":"2022-09-06T17:28:37.073258Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Model 1: Decision Tree\n\nIn fact, multiple studies (https://www.itm-conferences.org/articles/itmconf/pdf/2022/02/itmconf_icacs2022_01001.pdf) indicate using decision tree to classify spam is quite effective and\nefficient. Not to mention, it's a simple, efficient, scalable learning algorithm, and works for large sample sizes. It can also create complex nonlinear models and works for classification & regression problems.","metadata":{"id":"hEZUox_pffNs"}},{"cell_type":"code","source":"\nn_depth = np.arange(20,30,1)\naccs_valid = []\naccs_train = []\n\n# tuning with max_depth (had to batch it 'cause even colab wasn't able to run this tuning it on GPU)\nfor max_depth in n_depth:\n    clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, random_state=SEED)\n    clf_tree.fit(X_train, y_train)\n    valid_score = clf_tree.score(X_validation, y_validation)\n    accs_valid.append(valid_score)\n    train_score = clf_tree.score(X_train, y_train)\n    accs_train.append(train_score)\n    print(\"max_depth:\",max_depth , \" ; training score:\",train_score, \" ; validation score:\", valid_score)\n    del clf_tree\n    gc.collect\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:56:02.143146Z","iopub.execute_input":"2022-09-06T17:56:02.143742Z","iopub.status.idle":"2022-09-06T18:00:58.456618Z","shell.execute_reply.started":"2022-09-06T17:56:02.143707Z","shell.execute_reply":"2022-09-06T18:00:58.455415Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# plot\n# figure(figsize=(12, 6), dpi=80)\nplt.plot(n_depth,accs_train, label='train accuracy')\nplt.plot(n_depth,accs_valid, label='validation accuracy')\nplt.title(\"Decision Tree: accuracy with different max depth param \")\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:00:58.458649Z","iopub.execute_input":"2022-09-06T18:00:58.459050Z","iopub.status.idle":"2022-09-06T18:00:58.707478Z","shell.execute_reply.started":"2022-09-06T18:00:58.459011Z","shell.execute_reply":"2022-09-06T18:00:58.706581Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"idx_of_best_tree = np.argmax(np.array(accs_valid))\nmax_depth = n_depth[idx_of_best_tree]\nclf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, random_state=SEED)\nclf_tree.fit(X_train, y_train)\npred = clf_tree.predict(X_test) \nfpr = countFalsePositiveRate(pred,y_test)\nscore = clf_tree.score(X_test, y_test)\nprint(\"After tuning, the best performing tree has max depth of \", n_depth[idx_of_best_tree])\nprint(\"FPR of our best perfomring decision tree is:\", fpr ,\"%\")\nprint(\"And it has an entropy score of: \", score)\n","metadata":{"id":"oiB0eJY4iGve","execution":{"iopub.status.busy":"2022-09-06T18:10:19.664924Z","iopub.execute_input":"2022-09-06T18:10:19.665737Z","iopub.status.idle":"2022-09-06T18:10:19.874866Z","shell.execute_reply.started":"2022-09-06T18:10:19.665700Z","shell.execute_reply":"2022-09-06T18:10:19.873829Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"FPRs.append(fpr)\nscores.append(score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree analysis\n\nSince we couldn't reach FPR of lower than .2%, DT might be too simple (or we need much more tuning), but we'll try a less interpretable but easy to implement Random Forest.","metadata":{}},{"cell_type":"markdown","source":"# Model 2: Random Forest \n\nWe want to also utilize Random Forest because of our little data size as it is a powerful non-linear classifier, it's quick & easy to apply, and performs well, it is often used as quick fix when no huge training data for NN. \n\nThe implementation of the random forest algorithm in sklearn has many parameter. The most important ones are the number of trees used (n_estimators) and the maximal depth of a single tree (max_depth). Investigate how the number of used trees effects the training and testing accuracy.  ","metadata":{}},{"cell_type":"code","source":"n_depth = np.arange(26,38,1)\nn_trees = np.arange(33,35,1)\n\naccs_valid_hyper = []\naccs_train_hyper = []\nDT_params_hyper = []\nfor tree in n_trees:\n    for max_depth in n_depth:\n        model = RandomForestClassifier(criterion='entropy', n_estimators = tree, max_depth=max_depth, random_state=SEED)\n        model.fit(X_train, y_train)\n        valid_score = model.score(X_validation, y_validation)\n        accs_valid_hyper.append(valid_score)\n        train_score = model.score(X_train, y_train)\n        accs_train_hyper.append(train_score)\n        DT_params_hyper.append((tree,max_depth))\n        print(\"n_estimators:\",tree , \" ; max_depth:\",max_depth , \" ; training score:\",train_score, \" ; validation score:\", valid_score)\n        del model\n        gc.collect","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:33:28.257744Z","iopub.execute_input":"2022-09-06T18:33:28.258425Z","iopub.status.idle":"2022-09-06T18:36:26.350294Z","shell.execute_reply.started":"2022-09-06T18:33:28.258386Z","shell.execute_reply":"2022-09-06T18:36:26.349143Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"we see that 6 (or 12) estimate tree is the best","metadata":{}},{"cell_type":"code","source":"idx_of_best_hyperparam = np.argmax(np.array(accs_valid_hyper))\ntree, max_depth = DT_params_hyper[idx_of_best_hyperparam][0], DT_params_hyper[idx_of_best_hyperparam][1]\nmodel = RandomForestClassifier(criterion='entropy', n_estimators = tree, max_depth=max_depth, random_state=SEED)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test) \nscore = model.score(X_test, y_test)\nfpr = countFalsePositiveRate(pred,y_test)\nprint(\"After tuning, the best performing Random Forest has max depth of \", max_depth, \"and \", tree, \" estimators.\")\nprint(\"FPR of our best perfomring model is:\", fpr ,\"%\")\nprint(\"And it has an entropy score of: \", score)\ndel model\ngc.collect\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:36:26.352665Z","iopub.execute_input":"2022-09-06T18:36:26.353054Z","iopub.status.idle":"2022-09-06T18:36:32.996501Z","shell.execute_reply.started":"2022-09-06T18:36:26.353017Z","shell.execute_reply":"2022-09-06T18:36:32.995542Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"FPRs.append(fpr)\nscores.append(score)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest analysis\n\nWe see that even with something as simple as Random Forest, we still were able to achieve amazing result wiht some hyperparameter tuning.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Model 3: Linear classification\n","metadata":{}},{"cell_type":"code","source":"def learn_reg_ERM(X,y,lbda, print_loss): \n    max_iter = 200\n    e  = 0.001\n    alpha = 1.\n\n    w = np.random.randn(X.shape[1]);\n    for k in np.arange(max_iter):\n        h = np.dot(X,w)\n        l,lg = loss(h, y)\n        if print_loss:\n            print ('loss: {}'.format(np.mean(l)))\n        r,rg = reg(w, lbda) # r (regularization term) should be added to our prediction, we indirectly use regularization by using regularization gradient\n        # so we don't really use r, but use reg gradient in the next line\n        g = np.dot(X.T,lg) + rg # calculating minimization term\n        if (k > 0):\n            alpha = alpha * (np.dot(g_old.T,g_old))/(np.dot((g_old - g).T,g_old)) \n        w = w - alpha * g\n        if (np.linalg.norm(alpha * g) < e):\n            break\n        g_old = g\n    \n    return w\n\n\ndef loss(h, y): # y is actual, h is predicted\n    '''\n    hinge loss\n    is a loss function used for training classifiers\n    is used for \"maximum-margin\" classification\n    '''\n    # calculate hinge loss: max(0, 1 - yihi) \n    loss = np.maximum(0, 1 - y*h)\n\n    # gradients of the hinge loss wrt h: gi = -y if li > 0 else gi = 0\n    loss_gradient = -y * (loss > 0) \n\n    return loss, loss_gradient\n\ndef reg(w, lbda):   \n    '''\n    regularization\n    computes the  2 -regularizer and the gradient of the regularizer function at point  𝐰\n    '''\n    r = ( lbda / 2 ) * np.transpose(w) * w\n    g = lbda * w\n    return r, g\n\ndef predict(w,X):\n    '''\n     predicts the class label  𝑦  for a data point  𝐱  or a matrix  𝑋  of data points (row-wise) \n     for a previously trained linear model  𝐰\n    '''\n    preds = 2*(np.dot(X,w)>0) -1\n    return preds\n\ndef linearClassifier(X_train, y_train, X_valid, y_valid, lbda, print_loss):\n    print(\"---------- lbda: \", lbda, \" -----------\")\n    w = learn_reg_ERM(X_train, y_train, lbda = lbda, print_loss = print_loss) # if you choose .5 can have low accuracy (60)\n    preds = predict(w, X_train)\n    train_acc = 100.0 * np.sum(preds == y_train) / len(y_train)\n    print (\"Train Accuracy: \", train_acc)\n    preds = predict(w, X_valid,)\n    valid_acc = 100.0 * np.sum(preds == y_valid,) / len(y_valid,)\n    print (\"Validation Accuracy: \", valid_acc)\n    del w\n    gc.collect\n    return train_acc, valid_acc","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:51:52.768311Z","iopub.execute_input":"2022-09-06T18:51:52.768734Z","iopub.status.idle":"2022-09-06T18:51:52.783583Z","shell.execute_reply.started":"2022-09-06T18:51:52.768687Z","shell.execute_reply":"2022-09-06T18:51:52.782570Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_acc_lst = []\nvalid_acc_lst = []\nlbda_lst = [.005, .05, .5, 1, 5]\n\nfor lbda in lbda_lst:\n    train_acc, valid_acc = linearClassifier(X_train, y_train, X_validation, y_validation, lbda, print_loss=False)\n    train_acc_lst.append(train_acc)\n    valid_acc_lst.append(valid_acc)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:54:20.511634Z","iopub.execute_input":"2022-09-06T18:54:20.512432Z","iopub.status.idle":"2022-09-06T19:02:53.420612Z","shell.execute_reply.started":"2022-09-06T18:54:20.512389Z","shell.execute_reply":"2022-09-06T19:02:53.419408Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"plt.plot(lbda_lst, [a/100 for a in train_acc_lst], label='train acc')\nplt.plot(lbda_lst, [a/100 for a in valid_acc_lst], label='validation acc')\nplt.title(\"Linear Classification\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:05:58.325381Z","iopub.execute_input":"2022-09-06T19:05:58.326107Z","iopub.status.idle":"2022-09-06T19:05:58.587834Z","shell.execute_reply.started":"2022-09-06T19:05:58.326067Z","shell.execute_reply":"2022-09-06T19:05:58.586872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"idx_of_best_hyperparam = np.argmax(np.array(valid_acc_lst))\nlbda = lbda_lst[idx_of_best_hyperparam]\nw = learn_reg_ERM(X_train, y_train, lbda = lbda, print_loss = False) # if you choose .5 can have low accuracy (60)\npreds = predict(w, X_test)\ntest_acc = 100.0 * np.sum(preds == y_test) / len(y_test)\nfpr = countFalsePositiveRate(preds,y_test)\nprint(\"After tuning, the best performing mode with lambda of \", lbda)\nprint(\"FPR of our best perfomring model is:\", fpr ,\"%\")\nprint(\"And it has an accuracy of: \", test_acc , \"%\")\ndel w\ngc.collect","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:07:50.155582Z","iopub.execute_input":"2022-09-06T19:07:50.156049Z","iopub.status.idle":"2022-09-06T19:07:50.162629Z","shell.execute_reply.started":"2022-09-06T19:07:50.156004Z","shell.execute_reply":"2022-09-06T19:07:50.161634Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"FPRs.append(fpr)\nscores.append(test_acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Classification analysis\nAs we can see, Linear classification already reaches a much better FPR","metadata":{}},{"cell_type":"markdown","source":"# Model 4: Neural Network\n\nTrain the model using the training data, tune it using the validation data. Repeat this process until I had the 'best' parameters.\n","metadata":{}},{"cell_type":"code","source":"#tuning is unfortuantley not working very well on colab and I had to try different combinations in batches/fragments\n# but this is the main code structre I used to hyperparameter tuning\n\nfrom tensorflow import keras\n\ndef calc_class_weight(labels):\n    # calculate mu = 1/ (sum of all label values/ max number)\n    sum_of_labels = sum(labels.values())\n    mu = 1 / ( sum_of_labels /max(labels.values()))\n    class_weight = dict()\n    for key, value in labels.items():\n        score = math.log(mu * sum_of_labels / float(value))\n        class_weight[key] = score if score > 1.0 else 1.0\n    return class_weight\n\ny_validation_01 = np.copy(y_validation)\ny_validation_01[ y_validation_01 == -1] = 0\n\n# class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training onl.virtual_documents/).\n\n# set labels_dict\nlabels_dict = {0:y_train[y_train == -1].shape[0], 1:y_train[y_train == 1].shape[0]}\nclass_weight = calc_class_weight(labels_dict)\nprint(\"Class weight calculated to weight out imbalanced training data:\", class_weight)\n\n# because class weight can only take label from 0,1,2... we changed non spam label from -1 to 0\ny_train_01 = np.copy(y_train)\ny_train_01[y_train_01==-1] = 0\ny_train_01\n    \ntf.random.set_seed(SEED)\n\ndef NN(num_of_neurons,batch_size,epoch, lr):\n    print(\"num_of_neurons:\", num_of_neurons, \" ; batch_size:\", batch_size, \" ; epochs: \", epoch, \"; lr:\", lr)\n    \n    hyperparams = {\"num_of_neurons\": num_of_neurons, \"batch_size\": batch_size, \"epochs\": epoch, \"lr\": lr, \"dropout_rate\": .2}\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\", input_shape=([X_train.shape[-1]])))\n    # The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting\n    model.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\n    model.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\"))\n    model.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\n    model.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\"))\n    model.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\n    model.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\"))\n    model.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\n    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n    #  config the model with losses and metrics with\n    model.compile(\n        optimizer=keras.optimizers.Adam(hyperparams[\"lr\"]),\n        loss= \"binary_crossentropy\",\n        metrics=[\n            keras.metrics.Precision(name=\"precision\"),\n            keras.metrics.Recall(name=\"recall\"),\n            keras.metrics.Accuracy()\n        ],\n    )\n\n\n    print(\"Start training with the following hyperparameteres:\")\n    print(hyperparams)\n\n    model.fit(\n        X_train,\n        y_train_01,\n        batch_size=hyperparams[\"batch_size\"],\n        epochs=hyperparams[\"epochs\"],\n        verbose=0,\n        shuffle = True,\n        validation_data=(X_validation, y_validation),\n        class_weight=class_weight,\n    )\n    \n    loss, precision, recall, accuracy  = model.evaluate(X_validation,y_validation_01)\n    print(\"loss \", loss, \", precision \", precision, \" , recall \", recall,\" , accuracy \", accuracy)\n    print(\"-----------------------------------------------\")\n    del model\n    gc.collect\n    return loss, precision, recall, accuracy\n\n# hyper = []\n# losses, precisions, recalls, accuracys = [], [], [], []\n\n# num_of_neurons,batch_sizes,epochs, lrs = [16,32], [64,128], [200, 300], [.001, .01]\n# for num_of_neuron in num_of_neurons:\n#     for batch_size in batch_sizes:\n#         for epoch in epochs:\n#             for lr in lrs:\n#                 loss, precision, recall, accuracy = NN(num_of_neuron,batch_size,epoch, lr)\n#                 hyper.append((num_of_neuron,batch_size,epoch, lr))\n#                 losses.append(loss)\n#                 precisions.append(precision)\n#                 recalls.append(recall)\n#                 accuracys.append(accuracy)\n                \n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:40:35.633853Z","iopub.execute_input":"2022-09-06T19:40:35.634513Z","iopub.status.idle":"2022-09-06T19:40:35.655743Z","shell.execute_reply.started":"2022-09-06T19:40:35.634478Z","shell.execute_reply":"2022-09-06T19:40:35.654225Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#now\n# finding one model that's good perfomring enough\n\nhyperparams = {\"num_of_neurons\": 32, \"batch_size\": 128, \"epochs\": 300, \"lr\": .001, \"dropout_rate\": .2}\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\", input_shape=([X_train.shape[-1]])))\nmodel.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\nmodel.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\"))\nmodel.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\nmodel.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\"))\nmodel.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\nmodel.add(keras.layers.Dense(hyperparams[\"num_of_neurons\"], activation=\"relu\"))\nmodel.add(keras.layers.Dropout(hyperparams[\"dropout_rate\"]))\nmodel.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n\n#  config the model with losses and metrics with\nmodel.compile(\n    optimizer=keras.optimizers.Adam(hyperparams[\"lr\"]),#0.000004),#\n    loss= \"binary_crossentropy\",\n    metrics=[\n        keras.metrics.Precision(name=\"precision\"),\n        keras.metrics.Recall(name=\"recall\"),\n        keras.metrics.Accuracy()\n    ],\n)\n\nprint(\"Start training with the following hyperparameteres:\")\nprint(hyperparams)\n\nmodel.fit(\n    X_train,\n    y_train_01,\n    batch_size=hyperparams[\"batch_size\"],\n    epochs=hyperparams[\"epochs\"],\n    verbose=0,\n    shuffle = True,\n    validation_data=(X_validation, y_validation),\n    class_weight=class_weight,\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:40:35.657477Z","iopub.execute_input":"2022-09-06T19:40:35.657808Z","iopub.status.idle":"2022-09-06T19:47:21.447697Z","shell.execute_reply.started":"2022-09-06T19:40:35.657780Z","shell.execute_reply":"2022-09-06T19:47:21.446349Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"y_test_01 = np.copy(y_test)\ny_test_01[ y_test_01 == -1] = 0\n\npred = model.predict(X_test).reshape(-1)\nloss, precision, recall, accuracy  = model.evaluate(X_test,y_test_01)\nfpr = countFalsePositiveRate(pred,y_test_01,non_spam=0)\n\n# print(\"After tuning, the best performing mode with lambda of \", lbda)\nprint(\"FPR of our best perfomring model is:\", fpr ,\"%\")\nprint(\"And it has an accuracy of: \", accuracy)\n# remove stuff from RAM  \ndel model\ngc.collect","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:17:01.355646Z","iopub.execute_input":"2022-09-06T19:17:01.356573Z","iopub.status.idle":"2022-09-06T19:17:02.950798Z","shell.execute_reply.started":"2022-09-06T19:17:01.356536Z","shell.execute_reply":"2022-09-06T19:17:02.950083Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"FPRs.append(fpr)\nscores.append(accuracy)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NN analysis\n\nNN has proven to be by far the best perfomring in regards to FPR","metadata":{}},{"cell_type":"markdown","source":"# Precision/recall curve\n\nIt is used to evaluate the skill of a prediction model.\nPrecision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef calc_precision_recall(y_true, y_pred):\n    TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n    \n    try:\n        precision = TP / (TP + FP)\n    except:\n        precision = 1\n    \n    if np.isnan(precision):\n        precision = 1\n    \n    try:\n        recall = TP / (TP + FN)\n    except:\n        recall = 1\n\n    return precision, recall","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:51:07.314286Z","iopub.execute_input":"2022-09-06T19:51:07.315461Z","iopub.status.idle":"2022-09-06T19:51:07.324425Z","shell.execute_reply.started":"2022-09-06T19:51:07.315401Z","shell.execute_reply":"2022-09-06T19:51:07.323033Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"thresholds = np.linspace(0., 1., 20)\n\nprecisions = []\nrecalls = []\npred_classified = []\nfor t in thresholds:\n    tmp = np.copy(pred)\n    tmp[tmp<=t] = int(0)\n    tmp[tmp>=.5] = int(1)\n    tmp = tmp.reshape(-1)\n    tmp = np.array([int(t) for t in tmp])\n    pred_classified.append(tmp)\n    precisions.append(precision_score(y_test_01, tmp, average='binary'))\n    recalls.append(recall_score(y_test_01, tmp, average='binary'))\n\npred_classified","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:51:09.931508Z","iopub.execute_input":"2022-09-06T19:51:09.932093Z","iopub.status.idle":"2022-09-06T19:51:10.006231Z","shell.execute_reply.started":"2022-09-06T19:51:09.932056Z","shell.execute_reply":"2022-09-06T19:51:10.005314Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"precision_scores = []\nrecall_scores = []\n\nprobability_thresholds = np.linspace(0,1,num=100)\n\n# Find true positive / false positive rate for each threshold\nfor threshold in probability_thresholds:\n    \n    y_test_preds = []\n    \n    for prob in pred: #spam_prob\n        if prob > threshold:\n            y_test_preds.append(1)\n        else:\n            y_test_preds.append(0)\n    \n    precision, recall = calc_precision_recall(y_test, np.array(y_test_preds))\n        \n    precision_scores.append(precision)\n    recall_scores.append(recall)\n\n# get precison and recall at our chosen threshold 0.5\ny_test_preds = []\nchosen_threshold = .5\nfor prob in pred: #spam_prob\n    if prob > chosen_threshold:\n        y_test_preds.append(1)\n    else:\n        y_test_preds.append(0)\n    \nprecision_chosen, recall_chosen = calc_precision_recall(y_test_01, np.array(y_test_preds))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:49:55.763803Z","iopub.execute_input":"2022-09-06T19:49:55.764264Z","iopub.status.idle":"2022-09-06T19:49:55.769833Z","shell.execute_reply.started":"2022-09-06T19:49:55.764219Z","shell.execute_reply":"2022-09-06T19:49:55.769090Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nax.plot(recall_scores, precision_scores)\nplt.plot(recall_chosen,precision_chosen,'ro',label=\"At threshold 0.5\") \nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.legend(loc='center left')\nplt.title('Precision Recall Curve')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:26:33.916423Z","iopub.execute_input":"2022-09-06T01:26:33.916927Z","iopub.status.idle":"2022-09-06T01:26:34.139973Z","shell.execute_reply.started":"2022-09-06T01:26:33.916888Z","shell.execute_reply":"2022-09-06T01:26:34.137918Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"markdown","source":"# Compare results of different approaches","metadata":{}},{"cell_type":"code","source":"models = [\"Decision Tree\", \"Random Forest\", \"Linear Classification\", \"Neural Network\"]\n# because of RAM issue, I couldn't run all cells in one session, so I had to hardcode the result you've seen before here\nfprs = [0.4, 0.26666666666666666, 0.13333333333333333 ,0.06666666666666667]\nscores = [0.9873333333333333, 0.996, 0.988, 0.8793333172798157]","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:10:20.840304Z","iopub.execute_input":"2022-09-06T20:10:20.840784Z","iopub.status.idle":"2022-09-06T20:10:20.847171Z","shell.execute_reply.started":"2022-09-06T20:10:20.840748Z","shell.execute_reply":"2022-09-06T20:10:20.845915Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# plot\n# figure(figsize=(12, 6), dpi=80)\nplt.plot(models,fprs)\nplt.title(\"How each model perform on test w.r.t FPR\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"False Positive Rate (%)\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:12:15.807851Z","iopub.execute_input":"2022-09-06T20:12:15.808236Z","iopub.status.idle":"2022-09-06T20:12:16.027026Z","shell.execute_reply.started":"2022-09-06T20:12:15.808204Z","shell.execute_reply":"2022-09-06T20:12:16.025910Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# plot\n# figure(figsize=(12, 6), dpi=80)\nplt.plot(models,scores)\nplt.title(\"How each model perform on test w.r.t accuracy\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:12:40.322085Z","iopub.execute_input":"2022-09-06T20:12:40.322582Z","iopub.status.idle":"2022-09-06T20:12:40.544299Z","shell.execute_reply.started":"2022-09-06T20:12:40.322544Z","shell.execute_reply":"2022-09-06T20:12:40.543526Z"},"trusted":true},"execution_count":22,"outputs":[]}]}